---
title: "An√°lisis Genomas: E. coli"
author: "OMICs Analysis"
format: 
  html:
    toc: true
    toc-depth: 3
    theme: yeti
    toc-location: left-body
    smooth-scroll: true
    toc-title: "Contenido"
editor: visual
execute:
  engine: knitr
lang: es
---

![](images/logo_OAN_for_white.png){fig-align="center" width="300"}

# Introducci√≥n

Vamos a trabajar con ensambles de distintas cepas de *E. coli* hospedero *Bovino*.

**NOTA** ‚ö†Ô∏èüëÅÔ∏è‚Äçüó®Ô∏è: Para las figuras del curso se utilizaron todos los 110 genomas con hospedero Bovino. Para este tutorial generaremos un subconjunto de 16, ya incluidos genomas de referencia para patotipos EHEC y EPEC.

Para iniciar vamos a descargar los datos disponibles que existan usando `datasets` de NCBI, lo puedes instalar [aqu√≠](https://github.com/ncbi/datasets).

Con el siguiente c√≥digo vamos a descargar los genomas depositados en **NCBI** de *E. coli*

```{bash, eval=FALSE}
mkdir 1_data
cd 1_data

datasets \
summary genome taxon '562' \
--assembly-level complete\
--assembly-source refseq \
--as-json-lines > Escherichia_coli_562.json
```

```{bash, eval=FALSE}
dataformat tsv genome \
--inputfile Escherichia_coli_562.json > Escherichia_coli_562.tsv
```

Este conjunto de metadatos [`Escherichia_coli_562.tsv`](https://drive.google.com/file/d/19wuoj6hUPrJPmzf6d3KLo_OFpjnNykz4/view?usp=drive_link) nos servir√° para seleccionar los ensambles de genoma que necesitamos, este archivo cuenta con `58533` filas y `174` columnas.

Por ello es necesario utilizar la plataforma o m√©todo que m√°s prefieras, y quedarnos con un conjunto m√°s peque√±o de genomas para descargar.

En este tutorial haremos un ejemplo de parseo de datos con `R`.

```{r, message=FALSE, eval=FALSE}
#| code-fold: true
#| code-summary: "subsetting.R"

raw_tsv <- readr::read_tsv("0_data/Escherichia_coli_562.tsv")
dim(raw_tsv)


# Change colnames
colnames(raw_tsv) <- gsub(" ", ".", colnames(raw_tsv))

cols <- c("Assembly.BioSample.Accession", "Assembly.BioSample.Attribute.Name",
          "Assembly.BioSample.Attribute.Value", "Assembly.BioSample.Collected.by",
          "Assembly.BioSample.Collection.date", "Assembly.BioSample.Description.Comment",
          "Assembly.BioSample.Description.Organism.Name", "Assembly.BioSample.Description.Organism.Taxonomic.ID",
          "Assembly.BioSample.Description.Title", "Assembly.BioSample.Geographic.location",
          "Assembly.BioSample.Host", "Assembly.BioSample.Host.disease",
          "Assembly.BioSample.Isolation.source", "Assembly.BioSample.Last.updated",
          "Assembly.BioSample.Latitude./.Longitude", "Assembly.BioSample.Serovar",
          "Assembly.BioSample.Owner.Name", "Assembly.BioSample.Strain"
          
)



Refseq_for_taxon<- raw_tsv |>
  dplyr::select('Assembly.Accession',"Assembly.Level",
         'Organism.Name', starts_with('ANI'),starts_with('CheckM'), 
         cols,
         "Assembly.BioProject.Accession") |>
  dplyr::filter(ANI.Best.ANI.match.Organism == "Escherichia coli") |>
  dplyr::filter(grepl("GCF",Assembly.Accession)) |>
  dplyr::distinct()

colnames(Refseq_for_taxon) <- stringr::str_replace(colnames(Refseq_for_taxon), 
                                                   pattern = "Assembly.BioSample.",
                                                   replacement = "")

library(dplyr)


# Create the column 'Assembly.Biosample.Attribute' by combining the values and eliminating duplicates.
ecoli_df <- Refseq_for_taxon |>
  dplyr::group_by(Assembly.Accession) %>%
  summarise(
    Attribute = paste0(
      Attribute.Name, " : ", 
      Attribute.Value, 
      collapse = "; "
    ),
    .groups = "drop"
  ) %>%
  left_join(
    Refseq_for_taxon %>% 
      select(-Attribute.Name, -Attribute.Value) %>% 
      distinct(Assembly.Accession, .keep_all = TRUE),
    by = "Assembly.Accession"
  )


ecoli_subset <- ecoli_df %>% 
  select(where(~ n_distinct(.) > 1)) %>% 
  select(where(~ !all(is.na(.)))) %>% 
  filter(
    !is.na(CheckM.completeness),
    !is.na(Collection.date),
    !is.na(Host)
  )


ecoli_subset <- ecoli_subset %>% 
  mutate(find = CheckM.completeness >= 90 & CheckM.contamination < 5 & ANI.Best.ANI.match.ANI >= 96)


ecoli_subset <- ecoli_subset %>% 
  select(where(~ n_distinct(.) > 1)) %>% 
  select(where(~ !all(is.na(.)))) %>% 
  filter(
    !is.na(CheckM.completeness),
    !is.na(Collection.date),
    !is.na(Host)
  )


ecoli_subset$Host <- tolower(ecoli_subset$Host)

table(ecoli_subset$Host)

ecoli_subset2 <- ecoli_subset[ecoli_subset$Host == "bovine",]

ecoli_subset2$Geographic.location <- tolower(ecoli_subset2$Geographic.location)

ecoli_subset2[stringr::str_detect(ecoli_subset2$Geographic.location, pattern = "china"),
              "Geographic.location"] <- "china"

ecoli_subset2[stringr::str_detect(ecoli_subset2$Geographic.location, pattern = "usa"),
              "Geographic.location"] <- "usa"

ecoli_subset2 <- ecoli_subset2[ecoli_subset2$Geographic.location != "missing",]

# Quiero quedarme al azar con 4 de china y 4 de usa, y conservar a francia y suiza

ecoli_bovine <- ecoli_subset2 %>% 
  group_by(Geographic.location) %>% 
  slice_sample(n = 4) %>% 
  ungroup() %>% 
  select(Assembly.Accession, Strain, Geographic.location,
         Host, Owner.Name, Assembly.BioProject.Accession)

readr::write_tsv(ecoli_bovine, "0_data/Ecoli_bovineHost.tsv")

table(ecoli_subset2$Geographic.location)
```

En este tutorial este ser√° el archivo con el que trabajaremos [`Ecoli_bovineHost.tsv`](https://drive.google.com/file/d/19wuoj6hUPrJPmzf6d3KLo_OFpjnNykz4/view?usp=drive_link):

```{r, echo=FALSE, message=FALSE}

df <- readr::read_tsv("tables/Ecoli_bovineHost.tsv")

gt::gt(df)
```

### Descarga de archivos

Para descargar los archivos vamos a utilizar el siguiente c√≥digo:

```{bash, eval=FALSE}
for g in $(cut -f1 Ecoli_bovineHost.tsv | awk 'NR > 1'); do
        if [ -f $g.zip ]; then
            echo $g'.zip exists';
        else
            datasets download genome accession $g \ 
            --include genome --filename $g'.zip';
            sleep 2;
        fi
    done
```

Descomprimir los archivos

```{bash, eval=FALSE}
for g in $(cut -f1 Ecoli_bovineHost.tsv | awk 'NR > 1'); do
        unzip $g.zip -d $g;
        mv $g/ncbi_dataset/data/$g/*.fna $g.fna;
        rm -r $g/;
done

rm *.zip
```

Con esto ya contamos con un sub conjunto de datos listo para trabajar.

# Control de Calidad

## FastANI

```{bash, eval=FALSE}
fastANI \
--ql $HOME/2025_Demo_ecoli/1_QC/ANI/all_genomes.list \
--rl $HOME/2025_Demo_ecoli/1_QC/ANI/all_genomes.list \
-o ecoli_all_to_all.out\
--matrix
```

## BUSCO

```{bash, eval=FALSE}
source $HOME/bin/busco-env/bin/activate

genomes=$(ls $HOME/2025_Demo_ecoli/0_data/*.fna)

for g in $genomes; do
   
      id=$(echo $g | awk -F'/' '{print $NF}')

      /usr/bin/time -v \
        busco \
        -i "$g" \
        -l bacteria_odb10 \
        -m geno \
        -o 1_QC/busco/${id%.fna} \
        --cpu 8 \
        --force 
  done 
```

# Anotaci√≥n de genomas

## Prokka

`Prokka` es un anotador r√°pido y est√°ndar para genomas procariontes (bacterias, arqueas y virus asociados) que identifica caracter√≠sticas gen√≥micas (CDS, tRNAs, rRNAs, ncRNAs, etc.), las anota con nombres/IDs y genera archivos listos para an√°lisis posteriores o env√≠o a bases p√∫blicas.

```{bash, eval=FALSE}
genomes=$( ls $HOME/2025_Demo_ecoli/1_data/*.fna )

for g in $genomes; do
 prokka \
 $g \
 --cpus 6 \
 --outdir prokka/${id%.fna} \
 --genus Escherichia
done
```

-   `genomes=.../*.fna` ‚Äî crea la lista de archivos FASTA de contigs/consensos que ser√°n anotados.

-   El `for g in $genomes; do ... done` recorre cada ensamblaje.

-   `prokka $g` ‚Äî indica a Prokka el archivo de entrada (.fna/.fasta).

-   `--cpus 6` ‚Äî usa 6 hilos para acelerar pasos paralelizables (e.g., b√∫squedas).

-   `--outdir prokka/${id%.fna}` ‚Äî dirige la salida a una carpeta por genoma (nota: en el snippet que enviaste la variable `id` **no est√° definida** dentro del bucle; ver correcci√≥n m√°s abajo).

-   `--genus Escherichia` ‚Äî par√°metro opcional que ayuda a priorizar nombres/funciones (mejorar√° la selecci√≥n de nombres gen√©ricos). ([GitHub](https://github.com/tseemann/prokka?utm_source=chatgpt.com "Prokka - zap: Rapid prokaryotic genome annotation"))

**Principales archivos que genera Prokka y para qu√© sirven**

-   `<sample>.gff` ‚Äî *archivo maestro* en formato GFF3 que contiene las caracter√≠sticas anotadas (coordenadas, tipos, anotaciones). Usarlo para cargar en IGV/Artemis o como entrada a pipelines que esperan GFF. ([nf-co.re](https://nf-co.re/modules/prokka?utm_source=chatgpt.com "modules/prokka"), [stab.st-andrews.ac.uk](https://stab.st-andrews.ac.uk/wiki/index.php/Prokka?utm_source=chatgpt.com "Prokka - wiki"))

-   `<sample>.gbk` (o `.gbk.gz`) ‚Äî archivo en formato GenBank (√∫til para visualizaci√≥n/dep√≥sito y para herramientas que leen GenBank). ([nf-co.re](https://nf-co.re/modules/prokka?utm_source=chatgpt.com "modules/prokka"))

-   `<sample>.fna` ‚Äî la secuencia de nucle√≥tidos (contigs) usada/normalizada por Prokka (puede incluir cambios en nombres de secuencia). ([stab.st-andrews.ac.uk](https://stab.st-andrews.ac.uk/wiki/index.php/Prokka?utm_source=chatgpt.com "Prokka - wiki"))

-   `<sample>.ffn` ‚Äî secuencias de nucle√≥tidos de los genes codificantes (CDS) extra√≠das. ([stab.st-andrews.ac.uk](https://stab.st-andrews.ac.uk/wiki/index.php/Prokka?utm_source=chatgpt.com "Prokka - wiki"))

-   `<sample>.faa` ‚Äî traducciones (secuencias proteicas) de los CDS; √∫til para b√∫squedas funcionales (BLAST/DIAMOND), clustering o an√°lisis de pangenoma. ([stab.st-andrews.ac.uk](https://stab.st-andrews.ac.uk/wiki/index.php/Prokka?utm_source=chatgpt.com "Prokka - wiki"))

-   `<sample>.tbl` ‚Äî tabla sencilla con informaci√≥n de anotaci√≥n (√∫til para env√≠o a GenBank/ENA en algunos workflows). ([HPC NIH](https://hpc.nih.gov/apps/prokka.html?utm_source=chatgpt.com "prokka on Biowulf"))

-   `<sample>.sqn` (opcional) ‚Äî formato Sequin para enviar a NCBI (si se generan). ([HPC NIH](https://hpc.nih.gov/apps/prokka.html?utm_source=chatgpt.com "prokka on Biowulf"))

-   `PROKKA_*.log` / `prokka.err` ‚Äî logs que detallan programas internos y pasos realizados (√∫til para depuraci√≥n).

**Notas pr√°cticas y recomendaciones para Prokka**

-   Si tu entrada es multi-FASTA con m√∫ltiples contigs, Prokka generar√° un GenBank multi-registro con uno por contig.

-   Ajusta `--kingdom`, `--genus`, `--species`, `--strain` para mejorar la nomenclatura.

-   Prokka usa varias bases (p. ej. UniProt/Swiss-Prot) y heur√≠sticas; para anotaciones m√°s ¬´exhaustivas¬ª posteriores se pueden re-analizar las prote√≠nas (`.faa`) con herramientas especializadas. ([GitHub](https://github.com/tseemann/prokka?utm_source=chatgpt.com "Prokka - zap: Rapid prokaryotic genome annotation"))

------------------------------------------------------------------------

## Bakta

`Bakta` es un anotador dise√±ado para obtener anotaciones amplias, normalizadas y ¬´dbxref-ricas¬ª (m√∫ltiples referencias cruzadas) en genomas bacterianos y pl√°smidos. Est√° pensado para producir salidas estandarizadas (incluyendo JSON legible por m√°quina) y sORFs (small ORFs) adem√°s de las caracter√≠sticas est√°ndar (CDS, tRNA, rRNA, ncRNA, CRISPR, etc.). Usa una base de datos local y herramientas r√°pidas (p. ej. DIAMOND) para hacer b√∫squedas de homolog√≠a. ([Bakta Documentation](https://bakta.readthedocs.io/?utm_source=chatgpt.com "Bakta Documentation: Introduction"), [PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC8743544/?utm_source=chatgpt.com "Bakta: rapid and standardized annotation of bacterial ..."))

```         
source $HOME/bin/bakta-env/bin/activate
```

```{bash, eval=FALSE}
#| code-fold: true
#| code-summary: "Muestra el c√≥digo"
genomes=$(ls $HOME/2025_Demo_ecoli/1_data)

experiment_file="$HOME/2025_Demo_ecoli/Experiment_Design.tsv"
genomes=$(ls $HOME/2025_Demo_ecoli/1_data)

experiment_file="$HOME/2025_Demo_ecoli/Experiment_Design.tsv"

for g in $genomes; do
    id=$(basename "$g" | awk -F'/' '{print $NF}' | sed 's/.fna$//')

    strain=$(awk -F'\t' -v id="$id" '$1 == id {print $3}' "$experiment_file")

    bakta \
    --db $HOME/2025_Demo_ecoli/db \
    --output $HOME/2025_Demo_ecoli/2_annotation/bakta/${id} \
    --genus Escherichia \
    --strain "$strain" \
    --threads 6 \
    --force $HOME/1_data/${g}
done
```

-   `source .../bin/activate` ‚Äî activa el entorno (p. ej. virtualenv/conda) donde est√° Bakta.

-   `--db $HOME/.../db` ‚Äî ubicaci√≥n de la base de datos compacta de Bakta (necesaria; puede ocupar decenas de GB). ([Bakta Documentation](https://bakta.readthedocs.io/en/latest/cli/database.html?utm_source=chatgpt.com "Database - Bakta Documentation - Read the Docs"))

-   `--output ...` ‚Äî carpeta por genoma para guardar resultados.

-   `--genus` / `--strain` ‚Äî ayudan en la anotaci√≥n y en la generaci√≥n de nombres.

-   `--threads 6` ‚Äî n√∫mero de hilos para paralelizar b√∫squedas.

-   `--force` ‚Äî sobrescribir salidas existentes.

-   Bakta internamente usa b√∫squedas con DIAMOND/BLAST y una base SQLite con digest y anotaciones preasignadas (esto acelera y estandariza el proceso). ([Bakta Documentation](https://bakta.readthedocs.io/en/latest/cli/database.html?utm_source=chatgpt.com "Database - Bakta Documentation - Read the Docs"), [Bactopia](https://bactopia.github.io/v3.0.0/bactopia-tools/bakta/?utm_source=chatgpt.com "bakta - Bactopia"))

**Precauciones de recursos** ‚ö†Ô∏è\

Bakta requiere memoria y disco para su base y para los pasos intermedios; es habitual necesitar varios GB de RAM (tu nota de 8 GB es razonable como m√≠nimo) y tiempo cuando se procesan muchos genomas. Si tienes \>10 genomas, espera que el tiempo total aumente y planifica en un cl√∫ster o m√°quina con suficientes recursos. ([Bactopia](https://bactopia.github.io/v3.0.0/bactopia-tools/bakta/?utm_source=chatgpt.com "bakta - Bactopia"), [software.cqls.oregonstate.edu](https://software.cqls.oregonstate.edu/updates/bakta-1.8.0/?utm_source=chatgpt.com "Bakta 1.8.0 - CQLS Software Update List"))

**Archivos y salidas importantes que produce Bakta**

-   `annotation.gff` / `<id>.gff` ‚Äî GFF3 con anotaciones (coordenadas y atributos). ([Bakta Documentation](https://bakta.readthedocs.io/?utm_source=chatgpt.com "Bakta Documentation: Introduction"))

-   `<id>.gbk` ‚Äî GenBank (cuando se solicita o se genera). ([Bakta Documentation](https://bakta.readthedocs.io/?utm_source=chatgpt.com "Bakta Documentation: Introduction"))

-   `<id>.faa`, `<id>.ffn` ‚Äî prote√≠nas y secuencias nucleot√≠dicas de CDS (similares a Prokka). ([Bakta Documentation](https://bakta.readthedocs.io/?utm_source=chatgpt.com "Bakta Documentation: Introduction"))

-   `report.json` ‚Äî salida JSON rica en metadatos y dbxrefs (√∫til para pipelines automatizados). Bakta hace √©nfasis en producir JSON estandarizado que puede alimentar otras herramientas. ([Bakta Documentation](https://bakta.readthedocs.io/?utm_source=chatgpt.com "Bakta Documentation: Introduction"), [GitHub](https://github.com/oschwengers/bakta/discussions/300?utm_source=chatgpt.com "(Re-)Generating bakta outputs from the JSON only #300"))

-   `bakta.log` / registros ‚Äî salida est√°ndar y errores para depuraci√≥n. ([Bakta Documentation](https://bakta.readthedocs.io/en/latest/api/logs.html?utm_source=chatgpt.com "Logs - Bakta Documentation - Read the Docs"))

**Ventajas frente a Prokka**

-   Bakta genera metadatos m√°s ricos / dbxrefs y JSON preparado para downstream autom√°tico.

-   Usa una base precomprimida con mapeos a UniProt (y otras colecciones), lo que suele producir IDs m√°s estandarizados.

-   Prokka es muy r√°pido y ampliamente usado; Bakta tiende a ser m√°s exhaustivo y ¬´estandarizado¬ª pero con requisitos mayores de disco/DB. Para workflows, muchos usuarios ejecutan **Prokka** para anotaciones r√°pidas y **Bakta** cuando quieren anotaciones m√°s normalizadas y ricas en dbxrefs. ([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC8743544/?utm_source=chatgpt.com "Bakta: rapid and standardized annotation of bacterial ..."), [Bactopia](https://bactopia.github.io/v3.0.0/bactopia-tools/bakta/?utm_source=chatgpt.com "bakta - Bactopia"))

## Consejos para decidir cu√°l usar en tu pipeline

-   Si quieres **velocidad** y una anotaci√≥n est√°ndar r√°pida para muchas muestras: usa **Prokka** y luego procesa los `.faa` con herramientas funcionales (eg. EggNOG, InterProScan) seg√∫n necesites. ([GitHub](https://github.com/tseemann/prokka?utm_source=chatgpt.com "Prokka - zap: Rapid prokaryotic genome annotation"))

-   Si quieres **anotaciones m√°s ricas, normalizadas y con dbxrefs/JSON** para downstream autom√°tico (p. ej. bases de datos locales, comparaciones taxon√≥micas o reporte estandarizado), usa **Bakta** (pero planifica la DB y recursos). ([Bakta Documentation](https://bakta.readthedocs.io/?utm_source=chatgpt.com "Bakta Documentation: Introduction"), [PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC8743544/?utm_source=chatgpt.com "Bakta: rapid and standardized annotation of bacterial ..."))

## AMRfinderPlus

```{bash, eval=FALSE}
conda activate amrfinder

for faa in 2_annotation/prokka/*/*.faa; do
  sample=$(basename "${faa%%.*}")
  out="8_AMR/${sample}_amrfinder_prot.tsv"
  echo "Running amrfinder -p $faa -> $out"
  amrfinder -p "$faa" --plus --threads 4 -o "$out"
done

```

El `output` de cada genoma se ve de la siguiente manera:

```{r, echo=FALSE, message=FALSE}
df <- readr::read_tsv("tables/GCF_000005845_amrfinder_prot.tsv")

gt::gt(df)
```

## Virulence Factor DataBase

Primero vamos a descargar los FASTA de VFDB [aqu√≠](https://www.mgc.ac.cn/VFs/download.htm). Y vamos a crear la base de datos para poder usar posteriormente blastp. **NOTA**: esto lo puedes aplicar a cualquier otra base de datos!!

```{bash, eval=FALSE}
makeblastdb \
-in data/VFDB/VFDB_setB_pro.fas \
-dbtype prot \
-out data/VFDB/vfdb
```

Una vez que genramos la base de datos vamos a hacer un BLASTP de nuestros genomas `.faa`

```{bash, eval=FALSE}
BASE_DIR="$HOME/2025_Demo_ecoli"
samples=$(find "$BASE_DIR/2_annotation/prokka" -maxdepth 1 -type d -not -path "$BASE_DIR/2_annotation/prokka" -exec basename {} \;)

simult=2
counter=0
  for s in $samples; do
    if [[ "$counter" -lt "$simult" ]]; then
      protein_faa=$(find "$BASE_DIR/2_annotation/prokka/$s" -maxdepth 1 -type f -name "*.faa")
      if [ -n "$protein_faa" ]; then
        blastp -db "$BASE_DIR/data/VFDB/vfdb" \
          -query "$protein_faa" \
          -outfmt "6 qseqid sseqid pident qlen slen length evalue bitscore" \
          -max_target_seqs 1 -num_threads 4 > "$BASE_DIR/2_annotation/$s.out" &
        counter=$((counter+1))
        if [[ "$counter" -eq "$simult" ]]; then wait; counter=0; fi

    fi
  done
  wait
```

Esto genera una salida `.out` el cual se ve as√≠:

```{bash, eval=FALSE}
PICPPNBC_00001	VFG039381(gb|NP_819814)	34.545	820	464	55	0.94	32.7
PICPPNBC_00002	VFG002555(gb|WP_004194045)	30.952	310	346	84	0.031	35.8
PICPPNBC_00003	VFG014236(gb|WP_011333018)	30.400	428	231	125	0.061	35.0
PICPPNBC_00004	VFG016425(gb|WP_000780148)	35.849	98	459	53	2.0	26.9
PICPPNBC_00005	VFG013340(gb|WP_010945509)	29.213	258	224	89	7.2	27.7
PICPPNBC_00007	VFG049100(gb|WP_004184789)	31.481	317	371	54	3.6	29.3
PICPPNBC_00008	VFG038347(gb|WP_005320757)	45.714	195	292	35	1.4	29.3
PICPPNBC_00009	VFG023006(gb|WP_013827070)	25.362	188	383	138	0.95	30.0
PICPPNBC_00010	VFG005490(gb|WP_000472995)	30.556	237	1906	72	0.41	32.0
PICPPNBC_00011	VFG026864(gb|WP_005058381)	34.328	134	738	67	0.47	30.0
PICPPNBC_00012	VFG043573(gb|NP_219906)	61.333	638	660	600	0.0	719
PICPPNBC_00013	VFG015290(gb|WP_005763320)	44.444	376	488	63	8.46e-09	57.0
PICPPNBC_00014	VFG015888(gb|WP_003381691)	26.144	370	260	153	0.37	32.3
PICPPNBC_00015	VFG029464(gb|WP_014380395)	33.333	46	971	30	5.0	23.9
PICPPNBC_00018	VFG044083(gb|NP_250948)	26.946	299	312	167	1.23e-05	45.8
PICPPNBC_00019	VFG010667(gb|WP_015961409)	34.783	125	425	46	0.63	29.3
PICPPNBC_00020	VFG011263(gb|WP_003811903)	27.848	91	790	79	2.5	26.6
PICPPNBC_00021	VFG013269(gb|WP_010945211)	33.333	87	196	48	2.0	26.6
PICPPNBC_00022	VFG050230(gb|NP_219645)	31.746	313	285	63	2.6	29.6
PICPPNBC_00023	VFG007107(gb|WP_047863677)	21.673	938	677	263	0.017	38.9
PICPPNBC_00024	VFG006812(gb|WP_003729506)	32.450	164	154	151	1.53e-18	77.4
```

Una vez tenemos este archivo vamos a aplicarle ciertos filtros:

```{bash, eval=FALSE}
blast_vf_i=0
blast_vf_c=0.9
blast_vf_e="1e-50"

BASE_DIR="$HOME/2025_Demo_ecoli"
samples=$(find "$BASE_DIR/2_annotation/prokka" -maxdepth 1 -type d -not -path "$BASE_DIR/2_annotation/prokka" -exec basename {} \;)

for s in $samples; do
    if [ -f "$BASE_DIR/2_annotation/$s.out" ]; then
      awk -F '\t' '$3>='$blast_vf_i' && ($6/$5>='$blast_vf_c') && $(NF-1)<='$blast_vf_e' {print}' \
        "$BASE_DIR/2_annotation/$s.out" > "$BASE_DIR/2_annotation/$s.filtered"
    fi
done
```

Esto nos genera un versi√≥n del archivo filtrada `.filter`:

```{bash, eval=FALSE}
MFNFHKLG_00013	VFG043573(gb|NP_219906)	61.333	638	660	600	0.0	719
MFNFHKLG_00022	VFG034856(gb|WP_000181251)	100.000	473	473	473	0.0	970
MFNFHKLG_00031	VFG047720(gb|WP_012280231)	48.953	382	388	382	3.83e-130	378
MFNFHKLG_00032	VFG047710(gb|WP_014547360)	58.116	1073	1094	1072	0.0	1218
MFNFHKLG_00037	VFG030400(gb|WP_081465621)	32.186	517	507	494	1.31e-71	236
MFNFHKLG_00071	VFG009376(gb|WP_003893754)	46.701	201	197	197	1.04e-57	180
MFNFHKLG_00094	VFG013414(gb|WP_012054478)	77.303	305	305	304	2.18e-180	499
MFNFHKLG_00096	VFG023546(gb|YP_001703132)	37.101	901	772	814	6.49e-139	432
MFNFHKLG_00102	VFG046022(gb|WP_000157234)	100.000	400	400	400	0.0	809
MFNFHKLG_00103	VFG045984(gb|WP_001025155)	100.000	461	461	461	0.0	937
MFNFHKLG_00104	VFG045947(gb|WP_000360894)	100.000	146	146	146	9.84e-109	305
MFNFHKLG_00123	VFG023850(gb|WP_043955033)	35.922	308	315	309	2.30e-56	184
MFNFHKLG_00131	VFG004221(gb|WP_000924983)	32.783	424	391	424	2.97e-52	179
MFNFHKLG_00135	VFG021422(gb|WP_071586201)	62.529	866	881	854	0.0	1108
```

Con los datos de la base de datos `FASTA` podemos crear un archivos con el nombre de cada identificador, y asi sea m√°s sencillo anotar:

```{bash, eval=FALSE}
awk '/^>/ {if (NR>1) print ""; split($0, a, " "); id=a[1]; sub(/^>/, "", id); gene=""; desc=""; org=""; for(i=2; i<=length(a); i++) {if (a[i] ~ /^\(/) {gene=a[i]; sub(/^\(/, "", gene); sub(/\)$/, "", gene)} else if (a[i] ~ /^\[/) {org=a[i]; sub(/^\[/, "", org); sub(/\]$/, "", org)} else {desc=desc" "a[i]}} print id"\t"gene"\t"desc"\t"org} END {if (NR>0) print ""}' $HOME/2025_Demo_ecoli/data/VFDB/VFDB_setB_pro.fas > $HOME/2025_Demo_ecoli/data/VFDB/VFDB_setB_pro.tab

```

Este archivo `.tab` se ver√≠a algo as√≠:

```{r, echo=FALSE, message=FALSE}
df <- readr::read_tsv("tables/VFDB_setB_pro.tab") |> head(10)

gt::gt(df)
```

Podemos hacer un posterior parseo de datos con el lenguaje de programaci√≥n qu√© mas prefieras para llegar a algo asi para cada genoma:

```{r, echo=FALSE, message=FALSE}
df <- readr::read_tsv("tables/GCF_000010745.1.vf.tsv") |> head(10)

gt::gt(df)
```

# Pangenoma

Vamos a utilizar Roary, lo puedes descargar [aqu√≠](https://github.com/sanger-pathogens/Roary).

Preparamos los archivos `.gff` obtenidos de la anotaci√≥n con `Prokka` o `Bakta` para poder realizar el an√°lisis de pangenoma.

```{bash, eval=FALSE}
DIR_ORIGEN="$HOME/2025_Demo_ecoli/2_annotation/prokka"
DIR_DESTINO="$HOME/2025_Demo_ecoli/3_pangenome/data_genomes"
for carpeta in "$DIR_ORIGEN"/*; do
        if [[ -d "$carpeta" ]]; then
            nombre_carpeta=$(basename "$carpeta")

            # Buscar el primer archivo .gff o .gff3 en la carpeta 
            archivo_gff=$(find "$carpeta" -maxdepth 1 -type f \( -iname "*.gff" -o -iname "*.gff3" \) -print -quit || true)

            if [[ -n "$archivo_gff" ]]; then
                destino="$DIR_DESTINO/${nombre_carpeta}.gff"
                cp -f -- "$archivo_gff" "$destino"
                echo "Archivo $archivo_gff copiado como $destino"
            else
                echo "No se encontr√≥ archivo .gff/.gff3 en $carpeta"
            fi
        fi
    done
```

```{bash, eval=FALSE}
DIR_ORIGEN="$HOME/2025_Demo_ecoli/2_annotation/prokka"
DIR_DESTINO="$HOME/2025_Demo_ecoli/3_pangenome/data_genomes"

roary -f "$DIR_DESTINO/pangenome_prokka_prank" \
-e \
-p 8 \
-v "$DIR_DESTINO"/*.gff
```

# Filog√©nia

# Ap√©ndice

## Gr√°ficos de R
